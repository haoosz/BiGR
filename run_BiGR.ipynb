{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyEm7zzfcH2D"
      },
      "source": [
        "# Simple demo of [BiGR](https://arxiv.org/abs/2410.14672)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcGfcX1v1vFQ",
        "outputId": "b398172f-1a90-4bfc-ef6a-8876b9ca40fa"
      },
      "outputs": [],
      "source": [
        "# clone BiGR and install environments\n",
        "\n",
        "!git clone https://github.com/haoosz/BiGR.git\n",
        "import BiGR, os\n",
        "os.chdir('BiGR')\n",
        "os.environ['PYTHONPATH'] = '/env/python:/content/BiGR'\n",
        "# DiT imports:\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from torchvision.datasets.utils import download_url\n",
        "from hparams import args2H\n",
        "from llama.load_bigr import load_bigr\n",
        "from bae.binaryae import BinaryAutoEncoder, load_pretrain\n",
        "torch.set_grad_enabled(False)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cpu\":\n",
        "    print(\"GPU not found. Using CPU instead.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "FRAMD3UI-ZXY"
      },
      "outputs": [],
      "source": [
        "# download\n",
        "def download_model(url, model_name):\n",
        "    os.makedirs('pretrained_models', exist_ok=True)\n",
        "    download_url(url, 'pretrained_models')\n",
        "    local_path = 'pretrained_models/' + model_name\n",
        "    return local_path\n",
        "\n",
        "def find_model(args, model):\n",
        "    model, size, code_dim = model.strip().split('-')\n",
        "    code_dim = code_dim.split('d')[-1]\n",
        "    args.model = model + '-' + size\n",
        "    args.codebook_size = int(code_dim)\n",
        "    url_ckpt = f\"https://huggingface.co/haoosz/BiGR/resolve/main/gpt/bigr_{size}_d{code_dim}.pt\"\n",
        "    args.ckpt = download_model(url_ckpt, f\"bigr_{size}_d{code_dim}.pt\")\n",
        "\n",
        "    if code_dim == '24':\n",
        "      url_ckpt_bae = \"https://huggingface.co/haoosz/BiGR/resolve/main/bae/bae_d24/binaryae_ema_1000000.th\"\n",
        "      args.ckpt_bae = download_model(url_ckpt_bae, \"binaryae_ema_1000000.th\")\n",
        "    elif code_dim == '32':\n",
        "      url_ckpt_bae = \"https://huggingface.co/haoosz/BiGR/resolve/main/bae/bae_d32/binaryae_ema_950000.th\"\n",
        "      args.ckpt_bae = download_model(url_ckpt_bae, \"binaryae_ema_950000.th\")\n",
        "\n",
        "def args_parser():\n",
        "    # some hyperparameters\n",
        "    parser = argparse.ArgumentParser()\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    args.image_size = args.img_size = 256\n",
        "    args.num_classes = 1000\n",
        "    args.dataset = 'custom'\n",
        "    args.norm_first = True\n",
        "    args.cls_token_num = 1\n",
        "    args.dropout_p = 0.1\n",
        "    args.token_dropout_p = 0.0\n",
        "    args.drop_path_rate = 0.0\n",
        "    args.use_adaLN = True\n",
        "    args.p_flip = True\n",
        "    args.focal = 0.0\n",
        "    args.alpha = -1\n",
        "    args.aux = 0.0\n",
        "    args.n_repeat = 1\n",
        "    args.n_sample_steps = 256\n",
        "    args.seq_len = 256\n",
        "    args.temperature = 1.0\n",
        "    args.cfg_schedule = 'constant'\n",
        "    args.gumbel_schedule = 'down'\n",
        "    args.infer_steps = 100\n",
        "    return args\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCG_yYaRXx3D",
        "outputId": "4e6cfa65-98d8-4995-9268-659dfc7ba952"
      },
      "outputs": [],
      "source": [
        "# model config\n",
        "model = \"BiGR-L-d24\" #@param [\"BiGR-L-d24\", \"BiGR-XL-d24\", \"BiGR-XXL-d24\", \"BiGR-XXL-d32\"]\n",
        "\n",
        "args = args_parser()\n",
        "find_model(args, model)\n",
        "args_ae = args2H(args)\n",
        "\n",
        "# load B-AE\n",
        "bae = BinaryAutoEncoder(args_ae).to(device).eval()\n",
        "bae = load_pretrain(bae, args.ckpt_bae)\n",
        "print(f\"The code length of B-AE is set to {args_ae.codebook_size}\")\n",
        "\n",
        "# load BiGR\n",
        "model = load_bigr(args, args_ae, device).eval()\n",
        "print(f\"GPT Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"MLP Parameters in GPT: {sum(p.numel() for p in model.denoise_mlp.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can find the full list of ImageNet classes [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "YxFNjt5aRHRE",
        "outputId": "0fb7cc0f-03c7-46ab-854f-ac36bf574503"
      },
      "outputs": [],
      "source": [
        "seed = 0 #@param {type:\"raw\"}\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "cfg_scale = 2.5 #@param {type:\"slider\", min:2, max:5, step:0.1}\n",
        "num_sample_iter = 20 #@param {type:\"slider\", min:10, max:30, step:5}\n",
        "gumbel_temp = 0.01 #@param {type:\"slider\", min:0.00, max:0.03, step:0.005}\n",
        "\n",
        "# Labels to condition the model with (feel free to change):\n",
        "class_labels = [207, 360, 387, 974, 88, 979, 417, 279] #@param {type:\"raw\"}\n",
        "latent_size = args.image_size // 16\n",
        "\n",
        "n = len(class_labels)\n",
        "y = torch.tensor(class_labels, device=device)\n",
        "\n",
        "bs = y.shape[0]\n",
        "\n",
        "# sample!\n",
        "with torch.no_grad():\n",
        "    samples = model.generate_with_cfg(cond=y, max_new_tokens=latent_size ** 2, cond_padding=args.cls_token_num, num_iter=num_sample_iter,\n",
        "                out_dim=bae.codebook_size, cfg_scale=cfg_scale, cfg_schedule=args.cfg_schedule,\n",
        "                gumbel_temp=gumbel_temp, gumbel_schedule=args.gumbel_schedule, sample_logits=True, proj_emb=None)\n",
        "\n",
        "samples = samples.float().transpose(1,2).reshape(bs, -1, latent_size, latent_size)\n",
        "samples = bae.decode(samples)\n",
        "save_image(samples, 'sample.png', nrow=4, normalize=True, value_range=(0, 1))\n",
        "samples = Image.open(\"sample.png\")\n",
        "display(samples)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
